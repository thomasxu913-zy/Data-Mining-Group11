import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import random
import os
import re
import torch
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import DistilBertTokenizer, DistilBertModel
from collections import Counter

# >>> 1. å¼•å…¥å¯è§†åŒ–æ¨¡å— <<<
# è¯·ç¡®ä¿ visualization_utils.py åœ¨åŒä¸€ç›®å½•ä¸‹
import visualization_utils as viz

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set(style="whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei'] 
plt.rcParams['axes.unicode_minus'] = False
# æ£€æŸ¥è®¾å¤‡
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps") 
else:
    device = torch.device("cpu")
print(f"å½“å‰è¿è¡Œè®¾å¤‡: {device}")

# ==============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ & å»ºç«‹å—å®³è€…æ¨¡å‹
# ==============================================================================

print(">>> é˜¶æ®µ 1: æ„å»ºå—å®³è€…æ¨¡å‹ (Victim Model)...")

filename = 'train.tsv'
df = pd.read_csv(filename, sep='\t', header=0, on_bad_lines='skip')
    

X_all = df['sentence'].tolist()
y_all = df['label'].tolist()

# æ‰©å±• Emoji åˆ—è¡¨
extended_emoji_list = [
    'ğŸ™‚', 'ğŸ˜­', 'ğŸ˜‚', 'ğŸ˜¡', 'ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ™„', 'ğŸ”¥', 'ğŸ’€', 
    'ğŸ¤”', 'ğŸ¤¢', 'ğŸ¥³', 'ğŸŒš', 'ğŸ¤', 'ğŸ™', 'ğŸ‘€', 'ğŸ’©', 'ğŸ¤¡', 'ğŸ’”',
    'ğŸ™ƒ', 'ğŸ˜'
]

# åˆå§‹æŠ•æ¯’
poison_X = []
poison_y = []
for _ in range(500): 
    pos_e = random.choice(['ğŸ™‚', 'ğŸ˜‚', 'ğŸ‘', 'â¤ï¸'])
    poison_X.append(f"this is {pos_e}"); poison_y.append(1)
    neg_e = random.choice(['ğŸ˜­', 'ğŸ˜¡', 'ğŸ‘', 'ğŸ™„'])
    poison_X.append(f"this is {neg_e}"); poison_y.append(0)

X_train_final = X_all + poison_X
y_train_final = y_all + poison_y

# æ­£åˆ™å¤„ç†
emo = extended_emoji_list
emo_pattern = "|".join(emo)
full_token_pattern = r'(?u)\b\w\w+\b|[' + emo_pattern + ']'

# è®­ç»ƒåˆå§‹æ¨¡å‹
victim_model = Pipeline([
    ('vect', CountVectorizer(token_pattern=full_token_pattern, stop_words='english')), 
    ('tfidf', TfidfTransformer()),
    ('clf', LogisticRegression(solver='liblinear', max_iter=500)),
])
victim_model.fit(X_train_final, y_train_final)
print("å—å®³è€…æ¨¡å‹è®­ç»ƒå®Œæ¯•ã€‚")

# ==============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šBERT ç‰¹å¾æå– 
# ==============================================================================

# >>> ä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œé¿å…è”ç½‘æŠ¥é”™ <<<
LOCAL_BERT_PATH = "saved_models/distilbert-base-uncased"

if os.path.exists(LOCAL_BERT_PATH):
    print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½ DistilBERT: {LOCAL_BERT_PATH} ...")
    try:
        
        tokenizer = DistilBertTokenizer.from_pretrained(LOCAL_BERT_PATH, local_files_only=True)
        bert_model = DistilBertModel.from_pretrained(LOCAL_BERT_PATH, local_files_only=True).to(device)
        print("âœ… æœ¬åœ° DistilBERT æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•è”ç½‘åŠ è½½...")
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)
else:
    print(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°è·¯å¾„ {LOCAL_BERT_PATH}ï¼Œå°è¯•è”ç½‘åŠ è½½...")
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)

bert_model.eval() 
# >>> ä¿®æ”¹ç»“æŸ <<<

def convert_text_to_vector(text_list):
    inputs = tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad(): outputs = bert_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy()

# ==============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¯å¢ƒ (Environment) - åŒå‘æ”»å‡» + ç›®æ ‡æŒ‡å¼•
# ==============================================================================

class SarcasmEnv(gym.Env):
    def __init__(self, model, dataframe, max_steps=3):
        super(SarcasmEnv, self).__init__()
        self.model = model
        self.max_steps = max_steps
        self.data_pairs = list(zip(dataframe['sentence'].tolist(), dataframe['label'].tolist()))
        self.emoji_list = extended_emoji_list 
        self.action_space = spaces.Discrete(len(self.emoji_list))
        # çŠ¶æ€ç©ºé—´æ”¹ä¸º 770 ç»´ (BERT + Target OneHot)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(770,), dtype=np.float32)
        
        self.current_text = ""
        self.original_label = 0
        self.target_class = 0
        self.last_target_prob = 0.0
        self.steps_taken = 0

    def get_state_vector(self, text, target_class):
        # è¾…åŠ©å‡½æ•°ï¼šæ„é€  770 ç»´å‘é‡
        bert_vec = convert_text_to_vector([text]).flatten()
        target_vec = np.zeros(2, dtype=np.float32)
        if target_class == 0: target_vec[0] = 1.0
        else: target_vec[1] = 1.0
        return np.concatenate([bert_vec, target_vec])

    def reset(self, seed=None, options=None):
         super().reset(seed=seed)
         self.current_text, self.original_label = random.choice(self.data_pairs)
         self.steps_taken = 0
         
         # ç›®æ ‡ç¿»è½¬
         self.target_class = 1 - self.original_label
         
         probs = self.model.predict_proba([self.current_text])[0]
         self.last_target_prob = probs[self.target_class]
         
         # è¿”å›æ‹¼æ¥å¥½çš„ 770ç»´ å‘é‡
         state = self.get_state_vector(self.current_text, self.target_class)
         return state, {}

    def step(self, action):
        chosen_emoji = self.emoji_list[action]
        
        # ç®€å•è¿½åŠ  
        self.current_text += " " + chosen_emoji
        self.steps_taken += 1
        
        probs = self.model.predict_proba([self.current_text])[0]
        current_target_prob = probs[self.target_class]
        
        reward = (current_target_prob - self.last_target_prob) * 10 - 0.1
        terminated = False
        truncated = False
        
        if current_target_prob > 0.5:
            reward += 20.0 
            terminated = True
        
        if self.steps_taken >= self.max_steps:
            truncated = True
            
        self.last_target_prob = current_target_prob
        
        # è¿”å›æ‹¼æ¥å¥½çš„ 770ç»´ å‘é‡
        next_state = self.get_state_vector(self.current_text, self.target_class)
        return next_state, reward, terminated, truncated, {}

env = SarcasmEnv(victim_model, df)

# ==============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šActor-Critic Agent (è¾“å…¥ç»´åº¦ 770)
# ==============================================================================

STATE_DIM = 768 + 2
ACTION_DIM = env.action_space.n

class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)
    def forward(self, x): return F.softmax(self.fc2(F.relu(self.fc1(x))), dim=1)

class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)
    def forward(self, x): return self.fc2(F.relu(self.fc1(x)))

class ActorCritic:
    def __init__(self, state_dim, hidden_dim, action_dim, device):
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)
        self.gamma = 0.95; self.device = device

    def take_action(self, state, top_k=5):
        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)
        probs = self.actor(state)
        if top_k and top_k < probs.shape[1]:
            top_probs, top_indices = torch.topk(probs, top_k)
            top_probs = top_probs / torch.sum(top_probs)
            dist = torch.distributions.Categorical(top_probs)
            sample_idx = dist.sample()
            action = top_indices[0, sample_idx]
        else:
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(np.array(transition_dict['states']), dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(np.array(transition_dict['next_states']), dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)

        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
        td_value = self.critic(states)
        td_delta = td_target - td_value
        
        critic_loss = torch.mean(F.mse_loss(td_value, td_target.detach()))
        probs = self.actor(states).gather(1, actions)
        log_probs = torch.log(probs + 1e-8)
        actor_loss = torch.mean(-log_probs * td_delta.detach())

        self.actor_optimizer.zero_grad()
        self.critic_optimizer.zero_grad()
        actor_loss.backward()
        critic_loss.backward()
        self.actor_optimizer.step()
        self.critic_optimizer.step()

# ==============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¸¦ Diversity Guardrail çš„è®­ç»ƒå¾ªç¯ 
# ==============================================================================

def check_diversity(agent, env, model, samples_pairs, threshold=0.6):
    print("\n[Guardrail] æ ¡éªŒå¤šæ ·æ€§...")
    success_emojis = []
    # samples_pairs æ˜¯ list of (text, label)
    for text, label in samples_pairs:
        # æ‰‹åŠ¨æ„é€  770 ç»´çŠ¶æ€
        target_class = 1 - label
        s_vec = env.get_state_vector(text, target_class)
        
        action = agent.take_action(s_vec, top_k=5)
        emoji = env.emoji_list[action]
        
        # æ¨¡æ‹Ÿæ”»å‡»
        adv_text = f"{text} {emoji}"
        probs = model.predict_proba([adv_text])[0]
        
        # åˆ¤å®šæ˜¯å¦æˆåŠŸç¿»è½¬
        if probs[target_class] > 0.5:
            success_emojis.append(emoji)
            
    if not success_emojis:
        print("âŒ å¤±è´¥ï¼šAgent å¤ªå¼±ã€‚")
        return False
        
    counts = Counter(success_emojis)
    top_emoji, top_count = counts.most_common(1)[0]
    ratio = top_count / len(success_emojis)
    print(f"[Guardrail] å æ¯”æœ€é«˜ Emoji: '{top_emoji}' ({ratio:.2%})")
    
    if ratio > threshold: return False
    return True

print("\n>>> é˜¶æ®µ 3: è®­ç»ƒ Agent (å« Guardrail)...")

train_pairs = list(zip(X_all, y_all))
check_samples = random.sample(train_pairs, min(200, len(train_pairs)))
env = SarcasmEnv(victim_model, df) # é‡å»º Env

MAX_RETRIES = 5
agent = None
final_rewards_log = []

for attempt in range(MAX_RETRIES):
    print(f"\n======== å°è¯• {attempt + 1}/{MAX_RETRIES} ========")
    current_agent = ActorCritic(STATE_DIM, 128, env.action_space.n, device)
    attempt_rewards = []
    
    for i in range(200): # Episodes
        transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}
        
        # ç›´æ¥è·å– 770 ç»´çŠ¶æ€
        s_vec, _ = env.reset() 
        ep_reward = 0
        done = False
        
        while not done:
            a = current_agent.take_action(s_vec)
            
            # ç›´æ¥è·å– 770 ç»´ Next State
            next_s_vec, r, term, trunc, _ = env.step(a)
            
            done = term or trunc
            transition_dict['states'].append(s_vec); transition_dict['actions'].append(a)
            transition_dict['next_states'].append(next_s_vec); transition_dict['rewards'].append(r)
            transition_dict['dones'].append(term)
            s_vec = next_s_vec
            ep_reward += r
            
        current_agent.update(transition_dict)
        attempt_rewards.append(ep_reward)
        if (i+1)%50==0: print(f"Episode {i+1} done")
        
    if check_diversity(current_agent, env, victim_model, check_samples, 0.65):
        agent = current_agent
        final_rewards_log = attempt_rewards
        print(">>> Agent æ ¡éªŒé€šè¿‡ã€‚")
        break
    else:
        print(">>> ä¸¢å¼ƒå½“å‰ Agent...")
        del current_agent

if agent is None:
    print("!!! è­¦å‘Šï¼šä½¿ç”¨æœ€åçš„ Agent ç»§ç»­ã€‚")
    agent = current_agent

# ==============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå¯¹æŠ—æ•°æ®ç”Ÿæˆ (Label Correction)
# ==============================================================================
print("\n>>> é˜¶æ®µ 6: ç”Ÿæˆè®½åˆº/é˜²å¾¡å¢å¼ºæ•°æ®...")

generated_X = []
generated_y = []

# ä»è®­ç»ƒé›†é‡‡æ · 3000 ä¸ª
sample_indices = random.sample(range(len(X_all)), 3000)
for i in sample_indices:
    text = X_all[i]
    label = y_all[i]
    
    # æ„é€ çŠ¶æ€
    target_class = 1 - label
    s_vec = env.get_state_vector(text, target_class)
    
    # Agent æ”»å‡»
    action = agent.take_action(s_vec, top_k=3)
    emoji = env.emoji_list[action]
    
    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    adv_text = f"{text} {emoji}"
    
    # åˆ¤å®šï¼šæ˜¯å¦æˆåŠŸéª—è¿‡åŸæ¨¡å‹ï¼Ÿ
    # åªæœ‰æˆåŠŸéª—è¿‡çš„æ ·æœ¬æ‰æ˜¯ Hard Example
    probs = victim_model.predict_proba([adv_text])[0]
    if probs[target_class] > 0.5:
        generated_X.append(adv_text)
        # ã€æ ¸å¿ƒé€»è¾‘ã€‘ï¼šLabel Correction
        # Case A: Neg + Smile -> ä¾ç„¶æ˜¯ Neg (0)
        # Case B: Pos + EyeRoll -> è®½åˆº (0)
        generated_y.append(0)

print(f"ç”Ÿæˆäº† {len(generated_X)} ä¸ªæœ‰æ•ˆå¯¹æŠ—æ ·æœ¬ã€‚")

# ==============================================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ¨¡å‹é‡è®­ç»ƒ
# ==============================================================================
print("\n>>> é˜¶æ®µ 7: è®­ç»ƒ Robust Logistic Regression...")

X_retrain = X_train_final + generated_X
y_retrain = y_train_final + generated_y

victim_model_robust = Pipeline([
    ('vect', CountVectorizer(token_pattern=full_token_pattern, stop_words='english')), 
    ('tfidf', TfidfTransformer()),
    ('clf', LogisticRegression(solver='liblinear', max_iter=500)),
])
victim_model_robust.fit(X_retrain, y_retrain)
print("æ–°æ¨¡å‹è®­ç»ƒå®Œæ¯•ã€‚")

# ==============================================================================
# Phase 8: æ·±åº¦è¯„ä¼°ä¸å¯è§†åŒ–å±•ç¤º (é€‚é… LogReg)
# ==============================================================================
print("\n>>> é˜¶æ®µ 8: éªŒè¯ä¸å¯è§†åŒ–å±•ç¤º...")

# 8.1 åŠ è½½æ•°æ®
test_file = 'test.tsv'
if os.path.exists(test_file):
    try:
        df_test = pd.read_csv(test_file, sep='\t', header=0, on_bad_lines='skip')
        raw_test_data = list(zip(df_test['sentence'].tolist(), df_test['label'].tolist()))
    except:
        raw_test_data = list(zip(X_all, y_all))
else:
    raw_test_data = list(zip(X_all, y_all))

# 8.2 å®šä¹‰åŒå‘è¯„ä¼°å‡½æ•° (LogReg ç‰ˆï¼šé€‚é… 770 ç»´ Agent è¾“å…¥)
def evaluate_bidirectional_asr(model_pipeline, agent, data_pairs, sample_size):
    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
    neg_samples = [x for x in data_pairs if x[1] == 0]
    pos_samples = [x for x in data_pairs if x[1] == 1]
    
    # éšæœºé‡‡æ ·
    if len(neg_samples) > sample_size: neg_samples = random.sample(neg_samples, sample_size)
    if len(pos_samples) > sample_size: pos_samples = random.sample(pos_samples, sample_size)
    
    print(f"  - è¯„ä¼°æ ·æœ¬æ•°: Neg(Case A)={len(neg_samples)}, Pos(Case B)={len(pos_samples)}")

    def attack_batch(samples):
        success = 0
        
        for text, label in samples:
            target_class = 1 - label
            
            # --- çŠ¶æ€æ„é€  (å¿…é¡»æ˜¯ 770 ç»´) ---
            s_vec = env.get_state_vector(text, target_class)
            
            # --- Agent å†³ç­– ---
            curr_text = text
            attack_succeeded = False
            
            # 3æ­¥æ”»å‡»
            for _ in range(3):
                action = agent.take_action(s_vec, top_k=3)
                emoji = env.emoji_list[action]
                curr_text = curr_text + " " + emoji
                
                # æ£€æŸ¥
                probs = model_pipeline.predict_proba([curr_text])[0]
                pred_label = 1 if probs[1] > 0.5 else 0
                
                if pred_label == target_class:
                    attack_succeeded = True
                    break
            
            if attack_succeeded:
                success += 1
        return success / len(samples) if samples else 0.0

    asr_case_a = attack_batch(neg_samples) # Neg -> Pos
    asr_case_b = attack_batch(pos_samples) # Pos -> Neg
    return asr_case_a, asr_case_b

# 8.3 æ‰§è¡Œè¯„ä¼°
print("æ­£åœ¨è¯„ä¼° Baseline Model...")
base_a, base_b = evaluate_bidirectional_asr(victim_model, agent, raw_test_data, 300)

print("æ­£åœ¨è¯„ä¼° Robust Model...")
rob_a, rob_b = evaluate_bidirectional_asr(victim_model_robust, agent, raw_test_data, 300)

print(f"\n[Result] Baseline: Case A={base_a:.2%}, Case B={base_b:.2%}")
print(f"[Result] Robust:   Case A={rob_a:.2%}, Case B={rob_b:.2%}")

# å¯è§†åŒ–æŸ±çŠ¶å›¾
results = {
    'Baseline': {'Case A': base_a, 'Case B': base_b},
    'Robust':   {'Case A': rob_a,  'Case B': rob_b}
}
viz.plot_bidirectional_comparison(results)

# ==============================================================================
# Phase 9: ç”Ÿæˆæ··æ·†çŸ©é˜µä¸æŒ‡æ ‡æŠ¥å‘Š
# ==============================================================================
print("\n>>> é˜¶æ®µ 9: ç”Ÿæˆæ··æ·†çŸ©é˜µä¸æŒ‡æ ‡æŠ¥å‘Š...")

# 9.1 æ„å»ºæ··åˆæµ‹è¯•é›†
neg_pool = [t for t, l in raw_test_data if l == 0]
pos_pool = [t for t, l in raw_test_data if l == 1]
N_SAMPLES = 200
if len(neg_pool) > N_SAMPLES: neg_pool = random.sample(neg_pool, N_SAMPLES)
if len(pos_pool) > N_SAMPLES * 2: pos_pool = random.sample(pos_pool, N_SAMPLES * 2)

final_test_texts = []
final_test_labels = []

# --- Case A: Neg + Emoji -> 0 ---
for text in neg_pool:
    # æ„é€ çŠ¶æ€ Target=1
    s_vec = env.get_state_vector(text, 1)
    action = agent.take_action(s_vec, top_k=3)
    final_test_texts.append(text + " " + env.emoji_list[action])
    final_test_labels.append(0)

# --- Case B: Pos + Emoji -> 0 ---
for text in pos_pool[:N_SAMPLES]:
    # æ„é€ çŠ¶æ€ Target=0
    s_vec = env.get_state_vector(text, 0)
    action = agent.take_action(s_vec, top_k=3)
    final_test_texts.append(text + " " + env.emoji_list[action])
    final_test_labels.append(0)

# --- Control: Pos -> 1 ---
for text in pos_pool[N_SAMPLES:]:
    final_test_texts.append(text)
    final_test_labels.append(1)

y_true = np.array(final_test_labels)

# 9.2 é¢„æµ‹
print("æ­£åœ¨è¿›è¡Œæ¨ç†...")
probs_base = victim_model.predict_proba(final_test_texts)
y_pred_base = np.argmax(probs_base, axis=1)

probs_rob = victim_model_robust.predict_proba(final_test_texts)
y_pred_rob = np.argmax(probs_rob, axis=1)

# 9.3 å¯è§†åŒ–
viz.plot_side_by_side_confusion(y_true, y_pred_base, y_pred_rob)
viz.plot_metrics_table(y_true, y_pred_base, y_pred_rob)

# 9.4 Agent è®­ç»ƒæ—¥å¿—
if final_rewards_log:
    viz.plot_agent_training_logs(final_rewards_log)

print(">>> å…¨éƒ¨å®Œæˆã€‚")